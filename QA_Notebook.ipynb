{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read text from PDF\n",
    "##### Provide the path of PDF file before initialized everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data='data.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data.pdf'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data by reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_pdf(pdf):\n",
    "    text = \"\"\n",
    "    reader = PyPDF2.PdfReader(pdf)\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() or \"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_docx(docx):\n",
    "    doc = Document(docx)\n",
    "    text = \"\"\n",
    "    for paragraph in doc.paragraphs:\n",
    "        text += paragraph.text + \"\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The file can be any pdf or any docs. read the pdf/docs and go through each and every pages and extract the text'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def get_pdf_text(pdf_docs):\n",
    "    text =\"\"\n",
    "    for pdf in pdf_docs:\n",
    "        pdf_reader =PyPDF2.PdfReader(pdf_docs) \n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text'''\n",
    "\n",
    "\n",
    "\n",
    "'''The file can be any pdf or any docs. read the pdf/docs and go through each and every pages and extract the text'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(doc):\n",
    "    _, file_extension = doc.split('.')\n",
    "    print(file_extension)\n",
    "    if file_extension == 'pdf':\n",
    "        return extract_text_pdf(doc)\n",
    "    elif file_extension == 'docx':\n",
    "        return extract_text_docx(doc)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdf\n"
     ]
    }
   ],
   "source": [
    "text_data=extract(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert each text into chunks\n",
    "##### need to split the text using CharacterTextSplitter (which is basically present in Langchain.text_splitter) such that it should not increase token size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size =1300,\n",
    "    chunk_overlap =300,\n",
    "    length_function =len,\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(text_data)\n",
    "\n",
    "#taking the particular text & splitting based on information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cal hrs.)  Introduction  to System  Development  environment,  The Object  paradigm,  Objec - \n",
      "tory  S/W  development  process,  Object  Oriented  Modelling  using  UML,  Use case  diagrams, \n",
      "Class  diagram,  Interaction  diagram, Class  declaration, Method  implementation, Informa - \n",
      "tion hiding and Access modifiers, Class Hierarchy (inheritance), Polymorphism, Packages \n",
      "and Data  abstraction.  Practical  using  Rational  Rose  and OO Programming  using  Java/C++ \n",
      "Evaluatio n Method:  Practical  Examination  30%  and End Semester  Examina - \n",
      "tion 70% (Student must obtain at least 35% from the theory paper and the 30% \n",
      "from the practical paper)  7  COM213 α: Operating  Systems  (15 lecture  hrs. & 22.5  practical  hrs.)   Introduc - \n",
      "tion, Process Management, CPU Scheduling, Deadlocks, Memory Management, File -System \n",
      "Implementation, I/O Systems.  Practical oriented  \n",
      "Evaluation Method:  Practical Examination 30% and End  Semester Examina - \n",
      "tion 70% (Student must obtain at least 35% from the theory paper and 30% \n",
      "from the practical paper)  \n",
      " \n",
      "COM2141: Computer Architecture (15 lecture hrs.)  Number Systems, Boolean \n",
      "Algebra & K’maps, Logic Gates, Circuits Design, Memory Architect ure, CPU Architecture, \n",
      "Instruction Cycle, Instruction set.\n"
     ]
    }
   ],
   "source": [
    "print(chunks[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert chunks into vectors(Text Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#Import 'OpenAIEmbeddings class'\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "#initialize the connection of my db\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv()) \n",
    "\n",
    "# Retrieve the OpenAI API key from the environment variables\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Ensure the API key is available\n",
    "if OPENAI_API_KEY is None:\n",
    "    raise ValueError(\"OpenAI API key not found. Please set it in the .env file.\")\n",
    "\n",
    "# Initialize OpenAIEmbeddings with your API key\n",
    "embedding = OpenAIEmbeddings(openai_api_key = OPENAI_API_KEY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store in the vector db \n",
    "db = FAISS.from_texts(texts = chunks,embedding = embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"vectorstore/index.pkl\"):\n",
    "    # Load the existing vector store\n",
    "    db = FAISS.load_local(\"vectorstore\", embedding,allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    # Create a new vector store from the text chunks\n",
    "    db = FAISS.from_texts(texts=chunks, embedding=embedding)\n",
    "    # Save the newly created vector store locally\n",
    "    db.save_local(\"vectorstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
